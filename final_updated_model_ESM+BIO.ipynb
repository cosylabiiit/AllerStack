{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDCu3cbJtAjp",
        "outputId": "812f05d3-b4f1-43b0-f07b-232382f80ce3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Collecting lazypredict\n",
            "  Downloading lazypredict-0.2.12-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from lazypredict) (8.1.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lazypredict) (4.66.5)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (from lazypredict) (4.4.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (from lazypredict) (2.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Collecting nvidia-nccl-cu12 (from xgboost->lazypredict)\n",
            "  Downloading nvidia_nccl_cu12-2.22.3-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Downloading lazypredict-0.2.12-py2.py3-none-any.whl (12 kB)\n",
            "Downloading nvidia_nccl_cu12-2.22.3-py3-none-manylinux2014_x86_64.whl (190.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nccl-cu12, lazypredict\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.3.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nccl-cu12 2.22.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lazypredict-0.2.12 nvidia-nccl-cu12-2.22.3\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Collecting shap\n",
            "  Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.5)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.1)\n",
            "Collecting slicer==0.0.8 (from shap)\n",
            "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.43.0)\n",
            "Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (540 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.1/540.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.46.0 slicer-0.0.8\n",
            "Collecting Biopython\n",
            "  Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from Biopython) (1.26.4)\n",
            "Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Biopython\n",
            "Successfully installed Biopython-1.84\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Collecting keras==2.15.0\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.0 requires keras>=3.2.0, but you have keras 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0\n",
            "Collecting scikeras==0.10.0\n",
            "  Downloading scikeras-0.10.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras==0.10.0) (24.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras==0.10.0) (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras==0.10.0) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras==0.10.0) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras==0.10.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras==0.10.0) (3.5.0)\n",
            "Downloading scikeras-0.10.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas scikit-learn lazypredict\n",
        "!pip install pandas scikit-learn shap matplotlib\n",
        "!pip3 install Biopython\n",
        "!pip install tensorflow\n",
        "!pip install keras==2.15.0\n",
        "!pip install scikeras==0.10.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "CTQD92vktQdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "imTttYJHtSct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Allergen_Peptide_main.csv\")\n",
        "df['Sequence'] = df['Sequence'].str.replace('\\n', '')\n",
        "#remove space\n",
        "df['Sequence'] = df['Sequence'].str.replace(' ', '')\n",
        "# Remove rows with NaN values\n",
        "df.dropna(inplace=True)\n",
        "#converting objects to strings\n",
        "df['Sequence'] = df['Sequence'].astype(str)"
      ],
      "metadata": {
        "id": "uSYDIOFGtQZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the set of natural amino acids\n",
        "natural_amino_acids = set('ACDEFGHIKLMNPQRSTVWY')\n",
        "\n",
        "# function to check for non-natural amino acids\n",
        "def has_non_natural(seq):\n",
        "    non_natural_amino_acids = {'B', 'O', 'J', 'U', 'X', 'Z'}\n",
        "    return any(aa in non_natural_amino_acids for aa in seq)\n",
        "\n",
        "# Count and print the number of rows containing non-natural amino acids\n",
        "#contains_non_natural = df['Sequence'].apply(has_non_natural)\n",
        "#num_non_natural = contains_non_natural.sum()\n",
        "#print(\"Number of rows containing non-natural amino acids:\", num_non_natural)#\n",
        "\n",
        "# Drop rows with non-natural amino acids\n",
        "rows_to_delete = df[contains_non_natural].index\n",
        "df.drop(rows_to_delete, inplace=True)"
      ],
      "metadata": {
        "id": "hgPvq6U0tQYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BIoPython features"
      ],
      "metadata": {
        "id": "eswwo44KtiqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import Bio\n",
        "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
        "from google.colab import files\n",
        "\n",
        "def calculate_aac(sequence):\n",
        "    analysed_seq = ProteinAnalysis(sequence)\n",
        "    aac = analysed_seq.get_amino_acids_percent()\n",
        "    return aac\n",
        "\n",
        "def calculate_dipeptide_composition(sequence):\n",
        "    dipeptides = [a+b for a in 'ACDEFGHIKLMNPQRSTVWY' for b in 'ACDEFGHIKLMNPQRSTVWY']\n",
        "    composition = {dipeptide: 0 for dipeptide in dipeptides}\n",
        "    for i in range(len(sequence) - 1):\n",
        "        dipeptide = sequence[i:i+2]\n",
        "        if dipeptide in composition:\n",
        "            composition[dipeptide] += 1\n",
        "    total = sum(composition.values())\n",
        "    composition = {k: v / total for k, v in composition.items()}\n",
        "    return composition\n",
        "\n",
        "def calculate_pseudo_aac(sequence, lamda=10, weight=0.05):\n",
        "    analysed_seq = ProteinAnalysis(sequence)\n",
        "    aac = analysed_seq.get_amino_acids_percent()\n",
        "    theta = [analysed_seq.molecular_weight() / (i + 1) for i in range(lamda)]\n",
        "    pseudo_aac = {f\"PAAC_{i+1}\": weight * theta[i] for i in range(lamda)}\n",
        "    pseudo_aac.update(aac)\n",
        "    return pseudo_aac\n",
        "\n",
        "def calculate_physicochemical_properties(sequence):\n",
        "    analysed_seq = ProteinAnalysis(sequence)\n",
        "    properties = {\n",
        "        \"Molecular_Weight\": analysed_seq.molecular_weight(),\n",
        "        \"Aromaticity\": analysed_seq.aromaticity(),\n",
        "        \"Instability_Index\": analysed_seq.instability_index(),\n",
        "        \"Isoelectric_Point\": analysed_seq.isoelectric_point(),\n",
        "        \"Secondary_Structure_Fraction\": analysed_seq.secondary_structure_fraction()\n",
        "    }\n",
        "    return properties\n",
        "\n",
        "def extract_features(input_file):\n",
        "    output_file = input_file.rstrip('.csv') + \"_features.csv\"\n",
        "\n",
        "    df_in = pd.read_csv(input_file)\n",
        "    sequences = df_in['Sequence'].tolist()\n",
        "\n",
        "    features = []\n",
        "    for seq in sequences:\n",
        "        feature_set = {}\n",
        "        feature_set.update(calculate_aac(seq))\n",
        "        feature_set.update(calculate_dipeptide_composition(seq))\n",
        "        feature_set.update(calculate_pseudo_aac(seq))\n",
        "        feature_set.update(calculate_physicochemical_properties(seq))\n",
        "\n",
        "        features.append(feature_set)\n",
        "\n",
        "    features_df = pd.DataFrame(features)\n",
        "    result_df = pd.concat([df_in, features_df], axis=1)\n",
        "    result_df.to_csv(output_file, index=False)\n",
        "\n",
        "    return result_df, output_file\n",
        "\n",
        "# Generate features for the input file\n",
        "df_features, output_file = extract_features('/content/validate_negative_331.csv')\n",
        "\n",
        "# Display the output DataFrame (optional)\n",
        "print(df_features.head())\n",
        "\n",
        "# Download the output features CSV file\n",
        "files.download(output_file)"
      ],
      "metadata": {
        "id": "TsP0ToZrtQWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting some attached rows in feature generated file ( specifically for Secondary_structure_fraction feature)\n",
        "#Secondary_structure_fraction1 = alpha, Secondary_structure_fraction2 = beta, Secondary_structure_fraction 3= coil\n",
        "# Load the CSV file\n",
        "csv_file_path = \"/content/validate_negative_331_features.csv\"\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Split the \"Secondary_Structure_Fraction\" column into multiple columns\n",
        "df[['Secondary_Structure_Fraction1', 'Secondary_Structure_Fraction2', 'Secondary_Structure_Fraction3']] = df['Secondary_Structure_Fraction'].str.strip('()').str.split(',', expand=True).astype(float)\n",
        "\n",
        "# Drop the original \"Secondary_Structure_Fraction\"\n",
        "df = df.drop(columns=['Secondary_Structure_Fraction'])\n",
        "\n",
        "df.head()\n",
        "df.to_csv('/content/validate_negative_331_features_split.csv', index=False)"
      ],
      "metadata": {
        "id": "x6rC5K7OtQT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ESM features"
      ],
      "metadata": {
        "id": "oCp25anJ0JhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fair-esm\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import esm\n",
        "pip install git+https://github.com/facebookresearch/esm.git\n"
      ],
      "metadata": {
        "id": "Cr9rFCsmJYgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import esm\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load ESM-2 model\n",
        "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "model.eval()  # disables dropout for deterministic results\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('train_clusters_final.csv')\n",
        "\n",
        "# Extract sequences and sequence IDs\n",
        "sequences = df['Sequence'].tolist()\n",
        "sequence_ids = df['Sequence_ID'].tolist()\n",
        "\n",
        "# Prepare data in the required format for ESM\n",
        "data = list(zip(sequence_ids, sequences))\n",
        "\n",
        "# Set the batch size\n",
        "batch_size = 50\n",
        "\n",
        "# Initialize the list to store sequence representations\n",
        "sequence_representations = []\n",
        "\n",
        "# Process data in batches\n",
        "for start in tqdm(range(0, len(data), batch_size), desc=\"Processing ESM Batches\"):\n",
        "    end = start + batch_size\n",
        "    batch_data = data[start:end]\n",
        "\n",
        "    try:\n",
        "        # Use batch_converter to get labels, batch tokens, and lengths\n",
        "        batch_labels, batch_tokens, batch_lengths = batch_converter(batch_data)\n",
        "\n",
        "        # Ensure batch_tokens is converted to a list of numerical tokens\n",
        "        batch_tokens = torch.tensor(batch_tokens).to(device)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in batch_converter: {e}\")\n",
        "        continue\n",
        "\n",
        "    with torch.no_grad():\n",
        "        results = model(batch_tokens, repr_layers=[33])\n",
        "        token_embeddings = results[\"representations\"][33]\n",
        "\n",
        "    # Average the representations for each sequence to get fixed-length embeddings\n",
        "    for i, (seq_id, seq) in enumerate(batch_data):\n",
        "        seq_embedding = token_embeddings[i, 1:len(seq) + 1].mean(0).cpu().numpy()\n",
        "        sequence_representations.append(seq_embedding)\n",
        "\n",
        "# Assuming you want to add these embeddings to the DataFrame and save it\n",
        "# Add embeddings to DataFrame\n",
        "df['esm_embeddings'] = sequence_representations\n",
        "\n",
        "# Save DataFrame to CSV file\n",
        "df.to_csv('train_clusters_final_with_esm_embeddings.csv', index=False)\n"
      ],
      "metadata": {
        "id": "40Cls0kAJjjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame for the ESM embeddings\n",
        "esm_embeddings_df = pd.DataFrame(sequence_representations, columns=[f'esm_feature_{i}' for i in range(sequence_representations[0].shape[0])])\n",
        "esm_embeddings_df['Sequence_ID'] = sequence_ids\n",
        "\n",
        "# Merge ESM embeddings with the original DataFrame\n",
        "merged_df = pd.merge(df, esm_embeddings_df, on='Sequence_ID')\n",
        "\n",
        "# Save the merged DataFrame to a CSV file\n",
        "merged_df.to_csv('train_clusters_final_esm_embeddings.csv', index=False)\n",
        "\n",
        "print(\"ESM embeddings extraction and merging complete.\")"
      ],
      "metadata": {
        "id": "ww91R-AlJnpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BioPython feature Selection"
      ],
      "metadata": {
        "id": "_mu8Ilwptuqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# data loading\n",
        "train_data = pd.read_csv('/content/train_data_1727.csv')\n",
        "test_data = pd.read_csv('/content/test_data_1727.csv')\n",
        "validation_data = pd.read_csv('/content/validation_data_433.csv')\n",
        "\n",
        "# Separate features and labels\n",
        "X_train = train_data.drop(columns=[\"Label\"])\n",
        "y_train = train_data[\"Label\"]\n",
        "X_test = test_data.drop(columns=[\"Label\"])\n",
        "y_test = test_data[\"Label\"]\n",
        "validation_X = validation_data.drop(columns=[\"Label\"])\n",
        "validation_y = validation_data['Label']\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "validation_X_scaled = scaler.transform(validation_X)\n",
        "\n",
        "# Feature Engineering - Gradient Boosting Feature Importance\n",
        "gb = GradientBoostingClassifier(n_estimators=100, random_state=0)\n",
        "gb.fit(X_train_scaled, y_train)\n",
        "\n",
        "selector = SelectFromModel(gb, prefit=True, threshold='mean')\n",
        "X_train_selected = selector.transform(X_train_scaled)\n",
        "X_test_selected = selector.transform(X_test_scaled)\n",
        "validation_X_selected = selector.transform(validation_X_scaled)\n",
        "\n",
        "# Get selected feature names\n",
        "selected_features = X_train.columns[selector.get_support()]\n",
        "\n",
        "print(\"Selected Features (based on Gradient Boosting):\")\n",
        "print(selected_features)"
      ],
      "metadata": {
        "id": "VafdZlL4tQDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ESM feature selection"
      ],
      "metadata": {
        "id": "ygO6RVDXuIT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# data loading\n",
        "train_data = pd.read_csv('/content/train_data_esm.csv')\n",
        "test_data = pd.read_csv('/content/test_data_esm.csv')\n",
        "validation_data = pd.read_csv('/content/validation_data_433_esm.csv')\n",
        "\n",
        "# Separate features and labels\n",
        "X_train = train_data.drop(columns=[\"Label\"])\n",
        "y_train = train_data[\"Label\"]\n",
        "X_test = test_data.drop(columns=[\"Label\"])\n",
        "y_test = test_data[\"Label\"]\n",
        "validation_X = validation_data.drop(columns=[\"Label\"])\n",
        "validation_y = validation_data['Label']\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "validation_X_scaled = scaler.transform(validation_X)\n",
        "\n",
        "# Feature Engineering - Gradient Boosting Feature Importance\n",
        "gb = GradientBoostingClassifier(n_estimators=100, random_state=0)\n",
        "gb.fit(X_train_scaled, y_train)\n",
        "\n",
        "selector = SelectFromModel(gb, prefit=True, threshold='mean')\n",
        "X_train_selected = selector.transform(X_train_scaled)\n",
        "X_test_selected = selector.transform(X_test_scaled)\n",
        "validation_X_selected = selector.transform(validation_X_scaled)\n",
        "\n",
        "# Get selected feature names\n",
        "selected_features = X_train.columns[selector.get_support()]\n",
        "\n",
        "print(\"Selected Features (based on Gradient Boosting):\")\n",
        "print(selected_features)\n",
        "\n",
        "# Define a function to plot 3D t-SNE results with improved visualization and save the plot\n",
        "def plot_tsne_3d(X, y, title, save_path=None):\n",
        "    tsne = TSNE(n_components=3, random_state=0, perplexity=30, learning_rate=200)\n",
        "    X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # Create a custom colormap\n",
        "    cmap = ListedColormap(['#cc3f3f', '#498fc9'])\n",
        "\n",
        "    scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], X_tsne[:, 2], c=y, cmap=cmap, s=20, alpha=0.7)\n",
        "    #legend = ax.legend(*scatter.legend_elements(), title=\"Label\")\n",
        "    #ax.add_artist(legend)\n",
        "\n",
        "    #ax.set_title(title)\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, format='jpeg')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Plot and save 3D t-SNE for original features\n",
        "plot_tsne_3d(X_train_scaled, y_train, '3D t-SNE of All Features_ss_rb_ESM', save_path='3d_tsne_all_features_ss_rb_ESM.jpeg')\n",
        "\n",
        "# Plot and save 3D t-SNE for selected features\n",
        "plot_tsne_3d(X_train_selected, y_train, '3D t-SNE of Selected Features_ss_rb_ESM', save_path='3d_tsne_selected_features_ss_rb_ESM.jpeg')\n"
      ],
      "metadata": {
        "id": "mpWuNE5CuKkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model (ESM+BIO)"
      ],
      "metadata": {
        "id": "1nPeYmDKu5-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.12.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNk7CKTxkE09",
        "outputId": "0ec73aaf-145a-4cc3-e176-0ae32e8850b2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.12.0 in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.11.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.4.26)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.44.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.13.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, matthews_corrcoef, f1_score, confusion_matrix\n",
        "\n",
        "# Load datasets\n",
        "train_data = pd.read_csv('train_data_esm.csv')\n",
        "test_data = pd.read_csv('test_data_esm.csv')\n",
        "validation_data = pd.read_csv('validation_data_433_esm.csv')\n",
        "\n",
        "# Define selected features\n",
        "selected_features_main = [\n",
        "    'feature_2', 'feature_3', 'feature_10', 'feature_18', 'feature_26',\n",
        "    'feature_27', 'feature_38', 'feature_57', 'feature_65', 'feature_68',\n",
        "    'feature_90', 'feature_91', 'feature_101', 'feature_113', 'feature_119',\n",
        "    'feature_141', 'feature_149', 'feature_162', 'feature_163', 'feature_171',\n",
        "    'feature_173', 'feature_186', 'feature_190', 'feature_195', 'feature_197',\n",
        "    'feature_210', 'feature_222', 'feature_225', 'feature_230', 'feature_234',\n",
        "    'feature_235', 'feature_254', 'feature_265', 'feature_271', 'feature_276',\n",
        "    'feature_298', 'feature_313', 'feature_317', 'feature_322', 'feature_333',\n",
        "    'feature_356', 'feature_365', 'feature_382', 'feature_383', 'feature_396',\n",
        "    'feature_404', 'feature_414', 'feature_417', 'feature_440', 'feature_482',\n",
        "    'feature_492', 'feature_497', 'feature_515', 'feature_520', 'feature_526',\n",
        "    'feature_534', 'feature_546', 'feature_565', 'feature_568', 'feature_615',\n",
        "    'feature_643', 'feature_652', 'feature_653', 'feature_654', 'feature_657',\n",
        "    'feature_660', 'feature_690', 'feature_697', 'feature_704', 'feature_717',\n",
        "    'feature_719', 'feature_744', 'feature_784', 'feature_793', 'feature_800',\n",
        "    'feature_802', 'feature_805', 'feature_818', 'feature_830', 'feature_841',\n",
        "    'feature_842', 'feature_843', 'feature_853', 'feature_855', 'feature_858',\n",
        "    'feature_860', 'feature_869', 'feature_876', 'feature_878', 'feature_880',\n",
        "    'feature_895', 'feature_896', 'feature_914', 'feature_941', 'feature_943',\n",
        "    'feature_946', 'feature_979', 'feature_984', 'feature_988', 'feature_989',\n",
        "    'feature_992', 'feature_1003', 'feature_1007', 'feature_1013', 'feature_1015',\n",
        "    'feature_1025', 'feature_1031', 'feature_1048', 'feature_1051', 'feature_1065',\n",
        "    'feature_1069', 'feature_1072', 'feature_1077', 'feature_1085', 'feature_1093',\n",
        "    'feature_1112', 'feature_1119', 'feature_1122', 'feature_1144', 'feature_1145',\n",
        "    'feature_1149', 'feature_1158', 'feature_1162', 'feature_1163', 'feature_1182',\n",
        "    'feature_1185', 'feature_1213', 'feature_1223', 'feature_1228', 'feature_1233',\n",
        "    'feature_1245', 'feature_1246', 'feature_1248'\n",
        "]\n",
        "\n",
        "# Define the selected features for the QDA classifier\n",
        "selected_features_qda = [\n",
        "    'C', 'D', 'E', 'K', 'W', 'AA', 'AC', 'AE', 'AH', 'AT', 'CC', 'CG', 'CK',\n",
        "    'CN', 'DK', 'EC', 'EE', 'EQ', 'FE', 'FT', 'GA', 'GQ', 'GW', 'HF', 'HP',\n",
        "    'IE', 'IK', 'IN', 'KD', 'LE', 'LS', 'MG', 'ML', 'NW', 'NY', 'PP', 'RL',\n",
        "    'RV', 'ST', 'SV', 'VT', 'WC', 'WD', 'YF', 'YG', 'YR', 'YY', 'PAAC_5',\n",
        "    'PAAC_9', 'Aromaticity', 'Isoelectric_Point',\n",
        "    'Secondary_Structure_Fraction1', 'Secondary_Structure_Fraction3'\n",
        "]\n",
        "\n",
        "# Load QDA specific datasets\n",
        "train_data_qda = pd.read_csv('train_data_1727.csv')\n",
        "test_data_qda = pd.read_csv('test_data_1727.csv')\n",
        "validation_data_qda = pd.read_csv('validation_data_433.csv')\n",
        "\n",
        "# Prepare data for QDA\n",
        "X_train_qda_selected = train_data_qda[selected_features_qda]\n",
        "X_test_qda_selected = test_data_qda[selected_features_qda]\n",
        "validation_X_qda_selected = validation_data_qda[selected_features_qda]\n",
        "\n",
        "y_train_qda = train_data_qda['Label']\n",
        "y_test_qda = test_data_qda['Label']\n",
        "validation_y_qda = validation_data_qda['Label']\n",
        "\n",
        "# Prepare data for main models\n",
        "X_train_selected = train_data[selected_features_main]\n",
        "y_train = train_data['Label']\n",
        "X_test_selected = test_data[selected_features_main]\n",
        "y_test = test_data['Label']\n",
        "validation_X_selected = validation_data[selected_features_main]\n",
        "validation_y = validation_data['Label']\n",
        "\n",
        "# Define ANN model\n",
        "def create_ann_model(input_dim=196, dropout_rate=0.5):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define base models\n",
        "base_models = [\n",
        "    ('svc', Pipeline([('scaler', StandardScaler()), ('svc', SVC(probability=True))])),\n",
        "    ('qda', Pipeline([('scaler', StandardScaler()), ('qda', QuadraticDiscriminantAnalysis())])),\n",
        "    ('ann', KerasClassifier(build_fn=create_ann_model, input_dim=X_train_selected.shape[1])),\n",
        "    ('knn', Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier())]))\n",
        "]\n",
        "\n",
        "# Initialize arrays for meta-features\n",
        "meta_features_train = np.zeros((X_train_selected.shape[0], len(base_models)))\n",
        "meta_features_test = np.zeros((X_test_selected.shape[0], len(base_models)))\n",
        "meta_features_val = np.zeros((validation_X_selected.shape[0], len(base_models)))\n",
        "\n",
        "meta_features_train_qda = np.zeros((X_train_qda_selected.shape[0], 1))\n",
        "meta_features_test_qda = np.zeros((X_test_qda_selected.shape[0], 1))\n",
        "meta_features_val_qda = np.zeros((validation_X_qda_selected.shape[0], 1))\n",
        "\n",
        "# Train base models and get predictions\n",
        "for i, (name, model) in enumerate(base_models):\n",
        "    if name == 'qda':\n",
        "        model.fit(X_train_qda_selected, y_train_qda)\n",
        "        meta_features_train_qda[:, 0] = model.predict_proba(X_train_qda_selected)[:, 1]\n",
        "        meta_features_test_qda[:, 0] = model.predict_proba(X_test_qda_selected)[:, 1]\n",
        "        meta_features_val_qda[:, 0] = model.predict_proba(validation_X_qda_selected)[:, 1]\n",
        "    else:\n",
        "        model.fit(X_train_selected, y_train)\n",
        "        meta_features_train[:, i] = model.predict_proba(X_train_selected)[:, 1]\n",
        "        meta_features_test[:, i] = model.predict_proba(X_test_selected)[:, 1]\n",
        "        meta_features_val[:, i] = model.predict_proba(validation_X_selected)[:, 1]\n",
        "\n",
        "# Combine QDA meta-features with other base models\n",
        "meta_features_train_combined = np.hstack([meta_features_train, meta_features_train_qda])\n",
        "meta_features_test_combined = np.hstack([meta_features_test, meta_features_test_qda])\n",
        "meta_features_val_combined = np.hstack([meta_features_val, meta_features_val_qda])\n",
        "\n",
        "# Meta-model\n",
        "meta_model = XGBClassifier()\n",
        "meta_model.fit(meta_features_train_combined, y_train)\n",
        "\n",
        "# Predict using the meta-model\n",
        "meta_model_predictions_train = meta_model.predict(meta_features_train_combined)\n",
        "meta_model_predictions_test = meta_model.predict(meta_features_test_combined)\n",
        "meta_model_predictions_val = meta_model.predict(meta_features_val_combined)\n",
        "\n",
        "# Calculate performance metrics\n",
        "def print_metrics(name, y_true, predictions):\n",
        "    accuracy = accuracy_score(y_true, predictions)\n",
        "    mcc = matthews_corrcoef(y_true, predictions)\n",
        "    f1 = f1_score(y_true, predictions)\n",
        "    cm = confusion_matrix(y_true, predictions)\n",
        "    specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1]) if (cm[0, 0] + cm[0, 1]) > 0 else 0\n",
        "    print(f\"{name} - Accuracy:\", accuracy)\n",
        "    print(f\"{name} - MCC:\", mcc)\n",
        "    print(f\"{name} - F1 Score:\", f1)\n",
        "    print(f\"{name} - Specificity:\", specificity)\n",
        "\n",
        "print_metrics(\"Train Data\", y_train, meta_model_predictions_train)\n",
        "print_metrics(\"Test Data\", y_test, meta_model_predictions_test)\n",
        "print_metrics(\"Validation Data\", validation_y, meta_model_predictions_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-1ynCUou954",
        "outputId": "b6cfb7a0-8eae-4bc3-c92a-b38a010c7d0a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6878be04c381>:101: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  ('ann', KerasClassifier(build_fn=create_ann_model, input_dim=X_train_selected.shape[1])),\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87/87 [==============================] - 1s 4ms/step - loss: 0.5363 - accuracy: 0.7514\n",
            "87/87 [==============================] - 0s 2ms/step\n",
            "22/22 [==============================] - 0s 2ms/step\n",
            "27/27 [==============================] - 0s 3ms/step\n",
            "Train Data - Accuracy: 0.9985507246376811\n",
            "Train Data - MCC: 0.9971056379236809\n",
            "Train Data - F1 Score: 0.9985528219971056\n",
            "Train Data - Specificity: 0.9971014492753624\n",
            "Test Data - Accuracy: 0.958092485549133\n",
            "Test Data - MCC: 0.9161887976156524\n",
            "Test Data - F1 Score: 0.9580318379160636\n",
            "Test Data - Specificity: 0.9595375722543352\n",
            "Validation Data - Accuracy: 0.9699074074074074\n",
            "Validation Data - MCC: 0.9398551043917015\n",
            "Validation Data - F1 Score: 0.9697674418604652\n",
            "Validation Data - Specificity: 0.9745370370370371\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving base and meta models"
      ],
      "metadata": {
        "id": "N9xdU0oAt4tY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the meta-model\n",
        "with open('meta_model.pkl', 'wb') as f:\n",
        "    pickle.dump(meta_model, f)\n",
        "\n",
        "# Save the base models\n",
        "for name, model in base_models:\n",
        "    with open(f'{name}_model.pkl', 'wb') as f:\n",
        "        pickle.dump(model, f)"
      ],
      "metadata": {
        "id": "4kD3_lCzt77k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l-qZpVznlGc1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}